{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOTnOs4iVRo2M5On3Kv/5ll",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Eddy09246/Movies-Analysis-and-Linear-Regression/blob/main/Scraping_the_Data_with_BeautifulSoup.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Statements\n"
      ],
      "metadata": {
        "id": "xWJU4gWFGjIv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZHovXuBF6w2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Url of the first page"
      ],
      "metadata": {
        "id": "cq4pooTsImP3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://www.the-numbers.com/movie/budgets/all'"
      ],
      "metadata": {
        "id": "c7i3JkkyI1Xb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Scrape a page"
      ],
      "metadata": {
        "id": "ea7M92ScPI01"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_movie_data(url):\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()\n",
        "\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    table = soup.find('table')\n",
        "    data = []\n",
        "    if table:\n",
        "        rows = table.find_all('tr')\n",
        "        headers = [header.text.strip() for header in rows[0].find_all('th')]\n",
        "        for row in rows[1:]:\n",
        "            cols = row.find_all('td')\n",
        "            row_data = [col.text.strip() for col in cols]\n",
        "            data.append(row_data)\n",
        "\n",
        "    page_data = pd.DataFrame(data, columns=headers)\n",
        "    return page_data"
      ],
      "metadata": {
        "id": "Obn0JM9NPEpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Iterate through pages and store the data in CSV file ."
      ],
      "metadata": {
        "id": "--13gj6YOLCi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_movie_data = []\n",
        "next_page = 101\n",
        "while True:\n",
        "    movie_data = scrape_movie_data(url)\n",
        "\n",
        "    if movie_data is not None and not movie_data.empty:\n",
        "        all_movie_data.append(movie_data)\n",
        "        print(f\"Scraped data from {url}\")\n",
        "    else:\n",
        "        print(f\"No data found on {url}\")\n",
        "        break\n",
        "\n",
        "    # next_page_url:\n",
        "    url = f'https://www.the-numbers.com/movie/budgets/all/{next_page}'\n",
        "    next_page += 100\n",
        "\n",
        "# Concatenate data from all pages into a single DataFrame\n",
        "final_movie_data = pd.concat(all_movie_data, ignore_index=True)\n",
        "\n",
        "# Save the complete data to a CSV file\n",
        "final_movie_data.to_csv('all_movie_data.csv', index=False)\n",
        "print(\"All movie data successfully saved to all_movie_data.csv\")"
      ],
      "metadata": {
        "id": "9R2JG4tYLW6u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}